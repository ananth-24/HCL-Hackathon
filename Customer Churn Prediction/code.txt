# Necessary Libraries 
library(ggplot2)
library(lattice)
library(caret)
library(plyr)
library(pROC)

# load train.csv
churn_data <- read.csv('Customer-Churn.csv', stringsAsFactors = FALSE, na.strings = c("NA", ""))

# check data sample 
head(churn_data, n=5L)

# show columns info
str(churn_data)

# The missing data percentage by variable (exclude Survived)
sapply(churn_data[,-c(2)], function(x) round((sum(is.na(x))/length(x)*100),2))

# create the grouping function with the define intervals
CreateGrp <- function(tn){
    if (tn >= 0 & tn <= 12){
        return('0-12')
    }else if(tn > 12 & tn <= 24){
        return('12-24')
    }else if (tn > 24 & tn <= 48){
        return('24-48')
    }else if (tn > 48 & tn <=60){
        return('48-60')
    }else if (tn > 60){
        return('> 60')
    }
}
# apply the Group function to the tenure column
churn_data$GrpTenure <- sapply(churn_data$tenure,CreateGrp)
# set as factor the new column
churn_data$GrpTenure <- as.factor(churn_data$GrpTenure)
# check the frequency for each group bin
table(churn_data$GrpTenure)


churn_data$SeniorCitizen <- as.factor(
                            mapvalues(churn_data$SeniorCitizen,
                            from=c("0","1"),
                            to=c("No", "Yes"))
                         )
						 
						 
# convert to factor
churn_data$Churn  <- factor(churn_data$Churn)


# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"



# create the test dataset with only the testing columns
varsToKeep <- c('Churn','SeniorCitizen','Partner','Dependents','GrpTenure','PhoneService',
                'MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection',
                'TechSupport','StreamingTV','StreamingMovies','Contract','PaperlessBilling',
                'PaymentMethod','MonthlyCharges')
				
				
# let's split the dataset into two
churn_TrainTest <- churn_data[,varsToKeep]
split <- createDataPartition(churn_data$Churn, p = 0.75, list=FALSE)
churn_TrainModel <- churn_TrainTest[split,]
churn_TestModel <- churn_TrainTest[-split,]


trainX <- churn_TrainModel[,names(churn_TrainModel) != "Churn"]
preProcValues <- preProcess(x = trainX,method = c("center", "scale"))
preProcValues


# logistic regression
set.seed(7)
fit.glm <- train(Churn ~ ., data=churn_TrainModel, method="glm", metric=metric, trControl=control)

# SVM
set.seed(7)
fit.svm <- train(Churn ~ ., data=churn_TrainModel, method="svmRadial", metric=metric, trControl=control)

# Random Forest
set.seed(7)
fit.rf <- train(Churn ~ ., data=churn_TrainModel, method="rf", metric=metric, trControl=control)

# Gradient Boost Machine (GBM)
set.seed(7)
fit.gbm <- train(Churn ~ ., data=churn_TrainModel, method="gbm", 
                 metric=metric, trControl=control, verbose=FALSE)
				 
# summarize accuracy of models
results <- resamples(list(
    glm=fit.glm, 
    svm=fit.svm, 
    rf=fit.rf,
    gbm=fit.gbm
))
#summary(results)
# compare accuracy of models
dotplot(results)



AccCalc <- function(TestFit, name) {
    # prediction 
    churn_TestModelClean <- churn_TestModel
    churn_TestModelClean$Churn <- NA
    predictedval <- predict(TestFit, newdata=churn_TestModelClean)
    
    # summarize results with confusion matrix
    cm <- confusionMatrix(predictedval, churn_TestModel$Churn)
    
    # calculate accuracy of the model
    Accuracy<-round(cm$overall[1],2)
    acc <- as.data.frame(Accuracy)
 
    roc_obj <- roc(churn_TestModel$Churn, as.numeric(predictedval))
    acc$Auc <- auc(roc_obj)
    
    acc$FitName <- name
    return(acc)
}

accAll <- AccCalc(fit.glm, "glm")
accAll <- rbind(accAll, AccCalc(fit.svm, "svm"))
accAll <- rbind(accAll, AccCalc(fit.rf, "rf"))
accAll <- rbind(accAll, AccCalc(fit.gbm, "gbm"))
rownames(accAll) <- c()
arrange(accAll,desc(Accuracy))